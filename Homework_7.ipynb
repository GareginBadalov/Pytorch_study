{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0KqxC0ZuOiR",
        "outputId": "29bc3420-a342-40e8-f784-34eb4ca4d4bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "from string import punctuation\n",
        "from stop_words import get_stop_words\n",
        "# from pymorphy2 import MorphAnalyzer\n",
        "\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 1000\n",
        "max_len = 10\n",
        "num_classes = 1\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 512\n",
        "print_batch_n = 100"
      ],
      "metadata": {
        "id": "ApnARr73upZQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "# test_df = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "G6CAGR2Xurhh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.label.value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53QXbFrkuwXE",
        "outputId": "683af47e-56b3-4ba2-d366-645934432f1e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    29720\n",
              "1     2242\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid = train_test_split(train_df, test_size=0.25, random_state=42)\n",
        "X_train.shape, X_valid.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1-UmLvpuw8X",
        "outputId": "830ee243-e361-4b16-c3b8-4147a01ba61a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((23971, 3), (7991, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sw = set(get_stop_words(\"en\"))\n",
        "puncts = set(punctuation)\n",
        "# sw, puncts"
      ],
      "metadata": {
        "id": "M6RuMKtuu0rn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')\n",
        "morpher = WordNetLemmatizer()\n",
        "morpher.lemmatize('dogs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "l20sTZb7u96t",
        "outputId": "8f74ad45-c751-4210-fe43-4a6ed8f18853"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(txt):\n",
        "    txt = str(txt)\n",
        "    txt = \"\".join(c for c in txt if c not in puncts)\n",
        "    txt = txt.lower()\n",
        "    txt = [morpher.lemmatize(word) for word in txt.split() if word not in sw]\n",
        "    return \" \".join(txt)"
      ],
      "metadata": {
        "id": "eBfQMXpbvCOB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm \n",
        "tqdm.pandas()\n",
        "\n",
        "X_train.tweet = X_train.tweet.progress_apply(preprocess_text)\n",
        "X_valid.tweet = X_valid.tweet.progress_apply(preprocess_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8FKtwddvF2S",
        "outputId": "88aea141-aafc-4e9f-8d62-b54e45e7bc59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23971/23971 [00:01<00:00, 21389.46it/s]\n",
            "100%|██████████| 7991/7991 [00:00<00:00, 19926.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus = \" \".join(X_train.tweet)\n",
        "train_corpus = train_corpus.lower()\n",
        "tokens = word_tokenize(train_corpus)\n",
        "tokens[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ewRcvAhvJT9",
        "outputId": "0df8daa7-7d53-4d60-b2a5-dbc6c38554db"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['weekend', 'world', 'really', 'going', 'bonkers']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "tokens_filtered = [word for word in tokens if word.isalnum()]\n",
        "\n",
        "\n",
        "dist = FreqDist(tokens_filtered)\n",
        "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]  # вычитание 1 для padding\n",
        "len(tokens_filtered_top)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqKbkwzOvLhn",
        "outputId": "ecaf4f22-9c09-46db-aba1-811cc5fa3a47"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}"
      ],
      "metadata": {
        "id": "MGPV8yHavNfG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_sequence(text, maxlen):\n",
        "    \n",
        "    result = []\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens_filtered = [word for word in tokens if word.isalnum()] \n",
        "    for word in tokens_filtered:\n",
        "        if word in vocabulary:\n",
        "            result.append(vocabulary[word])\n",
        "\n",
        "    padding = [0] * (maxlen-len(result))\n",
        "    \n",
        "    return result[-maxlen:] + padding"
      ],
      "metadata": {
        "id": "gr_AnwMgvSNH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.asarray([text_to_sequence(text, max_len) for text in X_train.tweet])\n",
        "x_val = np.asarray([text_to_sequence(text, max_len) for text in X_valid.tweet])\n",
        "\n",
        "x_train.shape, x_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8Kic6QBvV8b",
        "outputId": "b6b77bca-c74f-41ce-950b-6f50f7ea814a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((23971, 10), (7991, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUFixedLen(nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, use_last=True):\n",
        "        super().__init__()\n",
        "        self.use_last = use_last\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        gru_out, ht = self.gru(x)\n",
        "       \n",
        "        if self.use_last:\n",
        "            last_tensor = gru_out[:,-1,:]\n",
        "        else:\n",
        "            # use mean\n",
        "            last_tensor = torch.mean(gru_out[:,:], dim=1)\n",
        "    \n",
        "        out = self.linear(last_tensor)\n",
        "        return torch.sigmoid(out)"
      ],
      "metadata": {
        "id": "7X6RLwN0vaS4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataWrapper(Dataset):\n",
        "    def __init__(self, data, target, transform=None):\n",
        "        self.data = torch.from_numpy(data).long() \n",
        "        self.target = torch.from_numpy(target).long() \n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "            \n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "IiktRyL9vene"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DataWrapper(x_train, X_train.label.values)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = DataWrapper(x_val, X_valid.label.values)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "8sDyTCEZvifN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Z6-OwFUGvk_G",
        "outputId": "261a4856-2a61-42d1-daac-d291dda9adba"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GRUFixedLen(max_words).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "jNwt-oR3vn_M"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_f1_score = torchmetrics.F1Score().to(device)\n",
        "test_f1_score = torchmetrics.F1Score().to(device)\n"
      ],
      "metadata": {
        "id": "2izIWIqJvqsG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "model.train()\n",
        "th = 0.5\n",
        "\n",
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "\n",
        "for epoch in range(epochs):  \n",
        "    running_items, running_right, all_f1_score_train = 0.0, 0.0, 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        # обнуляем градиент\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        train_f1_score(outputs.squeeze(), labels)\n",
        "        \n",
        "        loss = criterion(outputs, labels.float().view(-1, 1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # подсчет ошибки на обучении\n",
        "        loss = loss.item()\n",
        "        running_items += len(labels)\n",
        "        # подсчет метрики на обучении\n",
        "        pred_labels = torch.squeeze((outputs > th).int())\n",
        "        running_right += (labels == pred_labels).sum()\n",
        "        all_f1_score_train += train_f1_score.compute().item()\n",
        "        \n",
        "    # выводим статистику о процессе обучения (переводим модель в валидацию)\n",
        "    model.eval()\n",
        "    \n",
        "    print(f'Epoch [{epoch + 1}/{epochs}]. ' \\\n",
        "            f'Step [{i + 1}/{len(train_loader)}]. \\n' \\\n",
        "            f'Train: Loss: {loss:.3f}. ' \\\n",
        "            f'Acc: {running_right / running_items:.3f}, ' \\\n",
        "            f'f1_score_train: {all_f1_score_train / (i+1):.3f}, ')\n",
        "\n",
        "    \n",
        "    running_loss, running_items, running_right, all_f1_score_train = 0.0, 0.0, 0.0, 0.0\n",
        "    train_f1_score.reset()\n",
        "    train_loss_history.append(loss)\n",
        "\n",
        "        # выводим статистику на тестовых данных\n",
        "    test_running_right, test_running_total, test_loss, all_f1_score_test = 0.0, 0.0, 0.0, 0.0\n",
        "    for j, data in enumerate(val_loader):\n",
        "        \n",
        "        test_outputs = model(data[0].to(device))\n",
        "        test_labels = data[1].to(device)\n",
        "\n",
        "        # подсчет ошибки на тесте\n",
        "        test_loss = criterion(test_outputs, test_labels.float().view(-1, 1))\n",
        "        \n",
        "        # подсчет метрики на тесте\n",
        "        test_f1_score(test_outputs.squeeze(), test_labels)\n",
        "        \n",
        "        test_running_total += len(data[1])\n",
        "        pred_test_labels = torch.squeeze((test_outputs > th).int())\n",
        "        test_running_right += (test_labels == pred_test_labels).sum()\n",
        "        \n",
        "        all_f1_score_test += test_f1_score.compute().item()\n",
        "\n",
        "    test_loss_history.append(test_loss.item())\n",
        "    print(f'Test:  Loss: {test_loss:.3f}. Acc: {test_running_right / test_running_total:.3f},', end=' ')\n",
        "    \n",
        "    print(f'f1_score_test:  {all_f1_score_test / (j+1):.3f}\\n')\n",
        "    test_f1_score.reset()\n",
        "    \n",
        "    model.train()\n",
        "        \n",
        "print('Training is finished!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWCzxXkqvtUm",
        "outputId": "823653b8-ee6a-4010-a6e7-ae5a2268fe28"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]. Step [47/47]. \n",
            "Train: Loss: 0.238. Acc: 0.925, f1_score_train: 0.026, \n",
            "Test:  Loss: 0.042. Acc: 0.935, f1_score_test:  0.313\n",
            "\n",
            "Epoch [2/15]. Step [47/47]. \n",
            "Train: Loss: 0.239. Acc: 0.940, f1_score_train: 0.365, \n",
            "Test:  Loss: 0.033. Acc: 0.945, f1_score_test:  0.439\n",
            "\n",
            "Epoch [3/15]. Step [47/47]. \n",
            "Train: Loss: 0.158. Acc: 0.948, f1_score_train: 0.501, \n",
            "Test:  Loss: 0.826. Acc: 0.946, f1_score_test:  0.545\n",
            "\n",
            "Epoch [4/15]. Step [47/47]. \n",
            "Train: Loss: 0.132. Acc: 0.949, f1_score_train: 0.529, \n",
            "Test:  Loss: 0.115. Acc: 0.948, f1_score_test:  0.526\n",
            "\n",
            "Epoch [5/15]. Step [47/47]. \n",
            "Train: Loss: 0.190. Acc: 0.953, f1_score_train: 0.558, \n",
            "Test:  Loss: 0.014. Acc: 0.947, f1_score_test:  0.552\n",
            "\n",
            "Epoch [6/15]. Step [47/47]. \n",
            "Train: Loss: 0.128. Acc: 0.954, f1_score_train: 0.593, \n",
            "Test:  Loss: 0.035. Acc: 0.945, f1_score_test:  0.524\n",
            "\n",
            "Epoch [7/15]. Step [47/47]. \n",
            "Train: Loss: 0.094. Acc: 0.956, f1_score_train: 0.626, \n",
            "Test:  Loss: 0.020. Acc: 0.948, f1_score_test:  0.534\n",
            "\n",
            "Epoch [8/15]. Step [47/47]. \n",
            "Train: Loss: 0.097. Acc: 0.958, f1_score_train: 0.638, \n",
            "Test:  Loss: 0.015. Acc: 0.949, f1_score_test:  0.547\n",
            "\n",
            "Epoch [9/15]. Step [47/47]. \n",
            "Train: Loss: 0.102. Acc: 0.960, f1_score_train: 0.679, \n",
            "Test:  Loss: 0.202. Acc: 0.949, f1_score_test:  0.564\n",
            "\n",
            "Epoch [10/15]. Step [47/47]. \n",
            "Train: Loss: 0.103. Acc: 0.963, f1_score_train: 0.693, \n",
            "Test:  Loss: 0.493. Acc: 0.939, f1_score_test:  0.561\n",
            "\n",
            "Epoch [11/15]. Step [47/47]. \n",
            "Train: Loss: 0.074. Acc: 0.966, f1_score_train: 0.712, \n",
            "Test:  Loss: 0.005. Acc: 0.944, f1_score_test:  0.572\n",
            "\n",
            "Epoch [12/15]. Step [47/47]. \n",
            "Train: Loss: 0.076. Acc: 0.969, f1_score_train: 0.757, \n",
            "Test:  Loss: 0.003. Acc: 0.946, f1_score_test:  0.557\n",
            "\n",
            "Epoch [13/15]. Step [47/47]. \n",
            "Train: Loss: 0.106. Acc: 0.972, f1_score_train: 0.784, \n",
            "Test:  Loss: 0.006. Acc: 0.940, f1_score_test:  0.560\n",
            "\n",
            "Epoch [14/15]. Step [47/47]. \n",
            "Train: Loss: 0.094. Acc: 0.973, f1_score_train: 0.802, \n",
            "Test:  Loss: 1.222. Acc: 0.944, f1_score_test:  0.572\n",
            "\n",
            "Epoch [15/15]. Step [47/47]. \n",
            "Train: Loss: 0.052. Acc: 0.975, f1_score_train: 0.826, \n",
            "Test:  Loss: 0.179. Acc: 0.944, f1_score_test:  0.565\n",
            "\n",
            "Training is finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результат на мой взгляд получился неплохой, хоть и из-за недостатка времени, я не упел поиграть с моделью и предобработкой, а параметры подсмотрел в гугле и гите. "
      ],
      "metadata": {
        "id": "DDUCrR5txIPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "b-rld34kxZQy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}